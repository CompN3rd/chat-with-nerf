{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import open_clip\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-16\",  # e.g., ViT-B-16\n",
    "    pretrained=\"laion2b_s34b_b88k\",  # e.g., laion2b_s34b_b88k\n",
    "    precision=\"fp16\",\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "\n",
    "negatives = [\"object\", \"things\", \"stuff\", \"texture\"]\n",
    "with torch.no_grad():\n",
    "    tok_phrases = torch.cat([tokenizer(phrase) for phrase in negatives]).to(\n",
    "        \"cuda\"\n",
    "    )\n",
    "    neg_embeds = model.encode_text(tok_phrases)\n",
    "neg_embeds /= neg_embeds.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"book\"\n",
    "positives = [query]\n",
    "with torch.no_grad():\n",
    "    tok_phrases = torch.cat(\n",
    "        [tokenizer(phrase) for phrase in positives]\n",
    "    ).to(\"cuda\")\n",
    "    pos_embeds = model.encode_text(tok_phrases)\n",
    "pos_embeds /= pos_embeds.norm(dim=-1, keepdim=True)\n",
    "# use query to dot product with the point cloud -> centroids\n",
    "scales_list = torch.linspace(0.0, 1.5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy(\n",
    "    embed: torch.Tensor,\n",
    "    positive_id: int,\n",
    "    pos_embeds: Tensor,\n",
    "    neg_embeds: Tensor,\n",
    "    positive_words_length: int,\n",
    ") -> torch.Tensor:\n",
    "    phrases_embeds = torch.cat([pos_embeds, neg_embeds], dim=0)\n",
    "    p = phrases_embeds.to(embed.dtype)  # phrases x 512\n",
    "    output = torch.mm(embed, p.T)  # rays x phrases\n",
    "    positive_vals = output[..., positive_id : positive_id + 1]  # noqa E501\n",
    "    negative_vals = output[..., positive_words_length:]  # rays x N_phrase\n",
    "    repeated_pos = positive_vals.repeat(\n",
    "        1, 4\n",
    "    )  # rays x N_phrase\n",
    "\n",
    "    sims = torch.stack((repeated_pos, negative_vals), dim=-1)  # rays x N-phrase x 2\n",
    "    softmax = torch.softmax(10 * sims, dim=-1)  # rays x n-phrase x 2\n",
    "    best_id = softmax[..., 0].argmin(dim=1)  # rays x 2\n",
    "    return torch.gather(\n",
    "        softmax,\n",
    "        1,\n",
    "        best_id[..., None, None].expand(\n",
    "            best_id.shape[0], 4, 2\n",
    "        ),\n",
    "    )[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_file(load_config: str) -> dict:\n",
    "    hdf5_file = h5py.File(load_config, \"r\")\n",
    "    # batch_idx = 5\n",
    "    points = hdf5_file[\"points\"][\"points\"][:]\n",
    "    origins = hdf5_file[\"origins\"][\"origins\"][:]\n",
    "    directions = hdf5_file[\"directions\"][\"directions\"][:]\n",
    "\n",
    "    clip_embeddings_per_scale = []\n",
    "\n",
    "    clips_group = hdf5_file[\"clip\"]\n",
    "    for i in range(30):\n",
    "        clip_embeddings_per_scale.append(clips_group[f\"scale_{i}\"][:])\n",
    "\n",
    "    rgb = hdf5_file[\"rgb\"][\"rgb\"][:]\n",
    "    hdf5_file.close()\n",
    "    h5_dict = {\n",
    "        \"points\": points,\n",
    "        \"origins\": origins,\n",
    "        \"directions\": directions,\n",
    "        \"clip_embeddings_per_scale\": clip_embeddings_per_scale,\n",
    "        \"rgb\": rgb,\n",
    "    }\n",
    "    return h5_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dict = load_h5_file(\"/workspace/chat-with-nerf-dev/chat-with-nerf/data/scene0025_00/embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_phrases = len(positives)\n",
    "best_scale_for_phrases = [None for _ in range(n_phrases)]\n",
    "probability_per_scale_per_phrase = [\n",
    "    None for _ in range(n_phrases)\n",
    "]\n",
    "for i, scale in enumerate(scales_list):\n",
    "    clip_output = torch.from_numpy(\n",
    "        h5_dict[\"clip_embeddings_per_scale\"][i]\n",
    "    ).to(\"cuda\")\n",
    "    for i in range(n_phrases):\n",
    "        probs = get_relevancy(\n",
    "            embed=clip_output,\n",
    "            positive_id=i,\n",
    "            pos_embeds=pos_embeds,\n",
    "            neg_embeds=neg_embeds,\n",
    "            positive_words_length=1,\n",
    "        )\n",
    "        pos_prob = probs[..., 0:1]\n",
    "        if (\n",
    "            best_scale_for_phrases[i] is None\n",
    "            or pos_prob.max() > probability_per_scale_per_phrase[i].max()  # type: ignore\n",
    "        ):\n",
    "            best_scale_for_phrases[i] = scale\n",
    "            probability_per_scale_per_phrase[i] = pos_prob\n",
    "\n",
    "possibility_array = probability_per_scale_per_phrase[0].detach().cpu().numpy()  # type: ignore # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5068755 ],\n",
       "       [0.42661226],\n",
       "       [0.50202906],\n",
       "       ...,\n",
       "       [0.4464897 ],\n",
       "       [0.5149638 ],\n",
       "       [0.4339206 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibility_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possibility_array[possibility_array > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possibility_array[possibility_array < 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5245],\n",
       "        [0.5252],\n",
       "        [0.5385],\n",
       "        ...,\n",
       "        [0.5331],\n",
       "        [0.5226],\n",
       "        [0.5367]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
