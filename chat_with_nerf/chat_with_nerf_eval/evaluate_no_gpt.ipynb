{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import open_clip\n",
    "from torch import Tensor\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-16\",  # e.g., ViT-B-16\n",
    "    pretrained=\"laion2b_s34b_b88k\",  # e.g., laion2b_s34b_b88k\n",
    "    precision=\"fp16\",\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "\n",
    "negatives = [\"object\", \"things\", \"stuff\", \"texture\"]\n",
    "with torch.no_grad():\n",
    "    tok_phrases = torch.cat([tokenizer(phrase) for phrase in negatives]).to(\n",
    "        \"cuda\"\n",
    "    )\n",
    "    neg_embeds = model.encode_text(tok_phrases)\n",
    "neg_embeds /= neg_embeds.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_file(load_config: str) -> dict:\n",
    "    hdf5_file = h5py.File(load_config, \"r\")\n",
    "    # batch_idx = 5\n",
    "    points_nerfstudio = hdf5_file[\"points_nerfstudio\"][\"points_nerfstudio\"][:]\n",
    "    points_scannet = hdf5_file[\"points_scannet\"][\"points_scannet\"][:]\n",
    "    origins = hdf5_file[\"origins\"][\"origins\"][:]\n",
    "    directions = hdf5_file[\"directions\"][\"directions\"][:]\n",
    "\n",
    "    clip_embeddings_per_scale = []\n",
    "\n",
    "    clips_group = hdf5_file[\"clip\"]\n",
    "    for i in range(30):\n",
    "        clip_embeddings_per_scale.append(clips_group[f\"scale_{i}\"][:])\n",
    "\n",
    "    rgb = hdf5_file[\"rgb\"][\"rgb\"][:]\n",
    "    hdf5_file.close()\n",
    "    h5_dict = {\n",
    "        \"points_nerfstudio\": points_nerfstudio,\n",
    "        \"points_scannet\": points_scannet,\n",
    "        \"origins\": origins,\n",
    "        \"directions\": directions,\n",
    "        \"clip_embeddings_per_scale\": clip_embeddings_per_scale,\n",
    "        \"rgb\": rgb,\n",
    "    }\n",
    "    return h5_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy(\n",
    "    embed: torch.Tensor,\n",
    "    positive_id: int,\n",
    "    pos_embeds: Tensor,\n",
    "    neg_embeds: Tensor,\n",
    "    positive_words_length: int,\n",
    ") -> torch.Tensor:\n",
    "    phrases_embeds = torch.cat([pos_embeds, neg_embeds], dim=0)\n",
    "    p = phrases_embeds.to(embed.dtype)  # phrases x 512\n",
    "    output = torch.mm(embed, p.T)  # rays x phrases\n",
    "    positive_vals = output[..., positive_id : positive_id + 1]  # noqa E501\n",
    "    negative_vals = output[..., positive_words_length:]  # rays x N_phrase\n",
    "    repeated_pos = positive_vals.repeat(\n",
    "        1, 4\n",
    "    )  # rays x N_phrase\n",
    "\n",
    "    sims = torch.stack((repeated_pos, negative_vals), dim=-1)  # rays x N-phrase x 2\n",
    "    softmax = torch.softmax(10 * sims, dim=-1)  # rays x n-phrase x 2\n",
    "    best_id = softmax[..., 0].argmin(dim=1)  # rays x 2\n",
    "    return torch.gather(\n",
    "        softmax,\n",
    "        1,\n",
    "        best_id[..., None, None].expand(\n",
    "            best_id.shape[0], 4, 2\n",
    "        ),\n",
    "    )[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probability_query_property(query: str, h5_dict: dict):\n",
    "    positives = [query]\n",
    "    with torch.no_grad():\n",
    "        tok_phrases = torch.cat(\n",
    "            [tokenizer(phrase) for phrase in positives]\n",
    "        ).to(\"cuda\")\n",
    "        pos_embeds = model.encode_text(tok_phrases)\n",
    "    pos_embeds /= pos_embeds.norm(dim=-1, keepdim=True)\n",
    "    scales_list = torch.linspace(0.0, 1.5, 30)\n",
    "\n",
    "    n_phrases = len(positives)\n",
    "    prob_per_scale = []\n",
    "    for index, _ in enumerate(scales_list):\n",
    "        clip_output = torch.from_numpy(\n",
    "            h5_dict[\"clip_embeddings_per_scale\"][index]\n",
    "        ).to(\"cuda\")\n",
    "        # TODO: ensure i = 1\n",
    "        for i in range(n_phrases):\n",
    "            probs = get_relevancy(\n",
    "                embed=clip_output,\n",
    "                positive_id=i,\n",
    "                pos_embeds=pos_embeds,\n",
    "                neg_embeds=neg_embeds,\n",
    "                positive_words_length=1,\n",
    "            )\n",
    "            pos_prob = probs[..., 0:1]\n",
    "            # assert torch.sum(pos_prob) == 1\n",
    "            prob_per_scale.append(pos_prob)\n",
    "            # print(pos_prob)\n",
    "\n",
    "    return prob_per_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster(h5_dict, probability_over_all_points):\n",
    "    # Calculate the number of top values directly\n",
    "    top_count = int(probability_over_all_points.size * 0.005)\n",
    "    print(top_count)\n",
    "    print(probability_over_all_points.size)\n",
    "    # Find the indices of the top values\n",
    "    top_indices = np.argpartition(probability_over_all_points, -top_count)[\n",
    "        -top_count:\n",
    "    ]\n",
    "    # Fetch related data from the HDF5 dictionary\n",
    "    points = h5_dict[\"points_scannet\"]\n",
    "    # origins = self.h5_dict[\"origins\"]\n",
    "\n",
    "    top_positions = points[top_indices]\n",
    "    # top_origins = origins[top_indices]\n",
    "    top_values = probability_over_all_points[top_indices].flatten()\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=0.05, min_samples=15)  # Directly use values where possible\n",
    "    clusters = dbscan.fit(top_positions)\n",
    "    labels = clusters.labels_\n",
    "\n",
    "    # Find the cluster with the point closest to its centroid that has the highest value\n",
    "    best_cluster_value = -np.inf\n",
    "    best_cluster_id = None\n",
    "    for cluster_id in set(labels):\n",
    "        if cluster_id == -1:  # Ignore noise\n",
    "            continue\n",
    "\n",
    "        members = top_positions[labels == cluster_id]\n",
    "        values_for_members = top_values[labels == cluster_id]\n",
    "\n",
    "        # Calculate the centroid of the cluster\n",
    "        centroid = np.mean(members, axis=0)\n",
    "\n",
    "        # Compute the distance of all members to the centroid\n",
    "        distances_to_centroid = np.linalg.norm(members - centroid, axis=1)\n",
    "\n",
    "        # Find the index of the member closest to the centroid\n",
    "        closest_member_idx = np.argmin(distances_to_centroid)\n",
    "\n",
    "        # If this member has a better value than the current best, update\n",
    "        if values_for_members[closest_member_idx] > best_cluster_value:\n",
    "            best_cluster_value = values_for_members[closest_member_idx]\n",
    "            best_cluster_id = cluster_id\n",
    "    # For the best cluster, compute its centroid, bounding box and other desired values\n",
    "    members_of_best_cluster = top_positions[labels == best_cluster_id]\n",
    "    # values_for_best_cluster = top_values[labels == best_cluster_id]\n",
    "    # origins_for_best_cluster = top_origins[labels == best_cluster_id]\n",
    "\n",
    "    # Calculate the centroid of the best cluster\n",
    "    centroid_of_best = np.mean(members_of_best_cluster, axis=0)\n",
    "\n",
    "    # Determine the bounding box\n",
    "    # min_bounds = np.min(members_of_best_cluster, axis=0)\n",
    "    # max_bounds = np.max(members_of_best_cluster, axis=0)\n",
    "\n",
    "    sx = np.max(members_of_best_cluster[:, 0]) - np.min(\n",
    "        members_of_best_cluster[:, 0]\n",
    "    )\n",
    "    sy = np.max(members_of_best_cluster[:, 1]) - np.min(\n",
    "        members_of_best_cluster[:, 1]\n",
    "    )\n",
    "    sz = np.max(members_of_best_cluster[:, 2]) - np.min(\n",
    "        members_of_best_cluster[:, 2]\n",
    "    )\n",
    "\n",
    "    return centroid_of_best, (sx, sy, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bbox_corners(center, box_size):\n",
    "    sx, sy, sz = box_size\n",
    "    x_corners = [sx / 2, sx / 2, -sx / 2, -sx / 2, sx / 2, sx / 2, -sx / 2, -sx / 2]\n",
    "    y_corners = [sy / 2, -sy / 2, -sy / 2, sy / 2, sy / 2, -sy / 2, -sy / 2, sy / 2]\n",
    "    z_corners = [sz / 2, sz / 2, sz / 2, sz / 2, -sz / 2, -sz / 2, -sz / 2, -sz / 2]\n",
    "    corners_3d = np.vstack([x_corners, y_corners, z_corners])\n",
    "    corners_3d[0, :] = corners_3d[0, :] + center[0]\n",
    "    corners_3d[1, :] = corners_3d[1, :] + center[1]\n",
    "    corners_3d[2, :] = corners_3d[2, :] + center[2]\n",
    "    corners_3d = np.transpose(corners_3d)\n",
    "\n",
    "    return corners_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dict = load_h5_file(\"/workspace/chat-with-nerf-eval/data/scannet/scans/scene0025_00/h5_embedding/embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_per_scale = compute_probability_query_property(\"computer screen\", h5_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_per_scale_per_phrase: None | Tensor = None\n",
    "scales_list = torch.linspace(0.0, 1.5, 30)\n",
    "for i, scale in enumerate(scales_list):\n",
    "    pos_prob = prob_per_scale[i]\n",
    "    if (\n",
    "        probability_per_scale_per_phrase == None\n",
    "        or pos_prob.max() > probability_per_scale_per_phrase.max()  # type: ignore\n",
    "    ):\n",
    "        best_scale_for_phrases = scale\n",
    "        probability_per_scale_per_phrase = pos_prob\n",
    "\n",
    "possibility_array = probability_per_scale_per_phrase.detach().cpu().numpy().squeeze()  # type: ignore # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2514\n",
      "502854\n"
     ]
    }
   ],
   "source": [
    "center, box_size = find_cluster(h5_dict, possibility_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "conrners_3d = construct_bbox_corners(center, box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox(center, extents):\n",
    "    sx, sy, sz = extents\n",
    "    x_corners = [sx / 2, sx / 2, -sx / 2, -sx / 2, sx / 2, sx / 2, -sx / 2, -sx / 2]\n",
    "    y_corners = [sy / 2, -sy / 2, -sy / 2, sy / 2, sy / 2, -sy / 2, -sy / 2, sy / 2]\n",
    "    z_corners = [sz / 2, sz / 2, sz / 2, sz / 2, -sz / 2, -sz / 2, -sz / 2, -sz / 2]\n",
    "    corners_3d = np.vstack([x_corners, y_corners, z_corners])\n",
    "    corners_3d[0, :] = corners_3d[0, :] + center[0]\n",
    "    corners_3d[1, :] = corners_3d[1, :] + center[1]\n",
    "    corners_3d[2, :] = corners_3d[2, :] + center[2]\n",
    "    corners_3d = np.transpose(corners_3d)\n",
    "\n",
    "    lines = [\n",
    "        [0, 1], [1, 2], [2, 3], [3, 0],\n",
    "        [4, 5], [5, 6], [6, 7], [7, 4],\n",
    "        [0, 4], [1, 5], [2, 6], [3, 7]\n",
    "    ]\n",
    "    \n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]  # Red color for all lines\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(corners_3d)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return line_set\n",
    "\n",
    "\n",
    "def visualize_mesh_with_bboxes(mesh_path, bbox_centers, bbox_extents):\n",
    "    # Load mesh\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "\n",
    "    # Create a list to store all geometries (mesh + bboxes)\n",
    "    geometries = [mesh]\n",
    "    \n",
    "    # mesh.vertex_colors = o3d.utility.Vector3dVector([mesh_color for _ in range(len(mesh.vertices))])\n",
    "\n",
    "\n",
    "    # Create bounding boxes and add them to the list\n",
    "    bbox = create_bbox(bbox_centers, bbox_extents)\n",
    "    geometries.append(bbox)\n",
    "\n",
    "    # Visualize\n",
    "    o3d.visualization.draw_plotly(geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_path = \"/workspace/chat-with-nerf-eval/data/scannet/scans/scene0025_00/scene0025_00_vh_clean_2.ply\"\n",
    "bbox_centers = np.array([3.4272046, 2.0027978, 1.1025069])\n",
    "bbox_extents = np.array([0.48718548, 0.6294966, 0.34797996])\n",
    "visualize_mesh_with_bboxes(mesh_path, bbox_centers, bbox_extents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
